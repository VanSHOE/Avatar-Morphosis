# Image-Audio Lip Sync
This project takes in an image and an audio file and maps the audio to the image, creating a lip-synced video. It utilizes two existing open-source repositories: [Wav2Lip](https://github.com/Rudrabha/Wav2Lip) for audio-to-video lip syncing and [One-Shot Free-View Neural Talking-Head Synthesis](https://github.com/zhanglonghao1992/One-Shot_Free-View_Neural_Talking_Head_Synthesis) for generating a video from a single image.

This tool can be useful for a variety of applications, such as creating realistic talking head videos for presentations or video production, or for generating lip-synced animations in real-time applications. The process involves running the audio and image through each repository, combining the resulting video to generate the final output. The resulting image can be saved or further processed as needed.

Here is a video showcasing it in action: 

[![DEMO](https://img.youtube.com/vi/Ya2XJZz7N8w/0.jpg)](https://www.youtube.com/watch?v=Ya2XJZz7N8w) 

Template for frontend: https://berrydashboard.io/
